{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from datetime import date\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glob.glob('logs') != ['logs']:\n",
    "    os.mkdir('logs')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if glob.glob('weights') != ['weights']:\n",
    "    os.mkdir('weights')\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = 'ncf'\n",
    "today = date.today()\n",
    "log_formatter = logging.Formatter(\"%(asctime)s %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "log_file_name = \"./logs/{}_{}\".format(today, log_model)\n",
    "\n",
    "file_handler = logging.FileHandler(\"{}.log\".format(log_file_name))\n",
    "file_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. users: 6040, no. items: 3706\n"
     ]
    }
   ],
   "source": [
    "ratings_cols = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    "ratings = pd.read_csv('./ml-1m/ml-1m/ratings.dat', sep='::', engine='python', names=ratings_cols)\n",
    "\n",
    "num_users = ratings.UserID.unique().shape[0]\n",
    "num_items = ratings.MovieID.unique().shape[0]\n",
    "print('no. users: %d, no. items: %d' %(num_users, num_items))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, users, items, y):\n",
    "        self.x = torch.cat([\n",
    "            torch.LongTensor(users).unsqueeze(0).transpose(0, 1),\n",
    "            torch.LongTensor(items).unsqueeze(0).transpose(0, 1)\n",
    "        ], axis=1)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le1 = preprocessing.LabelEncoder()\n",
    "le2 = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     1197\n",
       "6     1287\n",
       "7     2804\n",
       "8      594\n",
       "9      919\n",
       "10     595\n",
       "11     938\n",
       "12    2398\n",
       "13    2918\n",
       "14    1035\n",
       "15    2791\n",
       "16    2687\n",
       "17    2018\n",
       "18    3105\n",
       "19    2797\n",
       "20    2321\n",
       "21     720\n",
       "22    1270\n",
       "23     527\n",
       "24    2340\n",
       "Name: MovieID, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.MovieID[5:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    le1.fit_transform(ratings.UserID),\n",
    "    le2.fit_transform(ratings.MovieID),\n",
    "    ratings.Rating.values\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   0, 2599]), tensor(5.))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=128, hidden_size=256):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.user_emb.weight.data.uniform_(0, 0.05)\n",
    "        self.item_emb.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, y = data[:, :1], data[:, 1:]\n",
    "        u, v = self.user_emb(x), self.item_emb(y)\n",
    "        uv = torch.cat((u, v), dim=1)\n",
    "        return self.mlp(uv.view(uv.size(0), -1)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print('DEVICE : %s' % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SUMMARY : NCF(\n",
      "  (user_emb): Embedding(6040, 256)\n",
      "  (item_emb): Embedding(3706, 256)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NCF(num_users, num_items, emb_size=256, hidden_size=256).to(device)\n",
    "print('MODEL SUMMARY :', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Epoch: 1\n",
      "Epoch 1: Average Loss: 1.0560\n",
      "Start Epoch: 2\n",
      "Epoch 2: Average Loss: 0.9490\n",
      "Start Epoch: 3\n",
      "Epoch 3: Average Loss: 0.9002\n",
      "Start Epoch: 4\n",
      "Epoch 4: Average Loss: 0.8535\n",
      "Start Epoch: 5\n",
      "Epoch 5: Average Loss: 0.8324\n",
      "Start Epoch: 6\n",
      "Epoch 6: Average Loss: 0.8114\n",
      "Start Epoch: 7\n",
      "Epoch 7: Average Loss: 0.7975\n",
      "Start Epoch: 8\n",
      "Epoch 8: Average Loss: 0.7755\n",
      "Start Epoch: 9\n",
      "Epoch 9: Average Loss: 0.7653\n",
      "Start Epoch: 10\n",
      "Epoch 10: Average Loss: 0.7514\n",
      "Start Epoch: 11\n",
      "Epoch 11: Average Loss: 0.7310\n",
      "Start Epoch: 12\n",
      "Epoch 12: Average Loss: 0.7208\n",
      "Start Epoch: 13\n",
      "Epoch 13: Average Loss: 0.7038\n",
      "Start Epoch: 14\n",
      "Epoch 14: Average Loss: 0.6906\n",
      "Start Epoch: 15\n",
      "Epoch 15: Average Loss: 0.6715\n",
      "Start Epoch: 16\n",
      "Epoch 16: Average Loss: 0.6535\n",
      "Start Epoch: 17\n",
      "Epoch 17: Average Loss: 0.6409\n",
      "Start Epoch: 18\n",
      "Epoch 18: Average Loss: 0.6229\n",
      "Start Epoch: 19\n",
      "Epoch 19: Average Loss: 0.6055\n",
      "Start Epoch: 20\n",
      "Epoch 20: Average Loss: 0.5907\n",
      "Start Epoch: 21\n",
      "Epoch 21: Average Loss: 0.5728\n",
      "Start Epoch: 22\n",
      "Epoch 22: Average Loss: 0.5573\n",
      "Start Epoch: 23\n",
      "Epoch 23: Average Loss: 0.5403\n",
      "Start Epoch: 24\n",
      "Epoch 24: Average Loss: 0.5210\n",
      "Start Epoch: 25\n",
      "Epoch 25: Average Loss: 0.5068\n",
      "Start Epoch: 26\n",
      "Epoch 26: Average Loss: 0.4918\n",
      "Start Epoch: 27\n",
      "Epoch 27: Average Loss: 0.4777\n",
      "Start Epoch: 28\n",
      "Epoch 28: Average Loss: 0.4612\n",
      "Start Epoch: 29\n",
      "Epoch 29: Average Loss: 0.4487\n",
      "Start Epoch: 30\n",
      "Epoch 30: Average Loss: 0.4367\n",
      "Start Epoch: 31\n",
      "Epoch 31: Average Loss: 0.4275\n",
      "Start Epoch: 32\n",
      "Epoch 32: Average Loss: 0.4147\n",
      "Start Epoch: 33\n",
      "Epoch 33: Average Loss: 0.4063\n",
      "Start Epoch: 34\n",
      "Epoch 34: Average Loss: 0.3957\n",
      "Start Epoch: 35\n",
      "Epoch 35: Average Loss: 0.3878\n",
      "Start Epoch: 36\n",
      "Epoch 36: Average Loss: 0.3837\n",
      "Start Epoch: 37\n",
      "Epoch 37: Average Loss: 0.3753\n",
      "Start Epoch: 38\n",
      "Epoch 38: Average Loss: 0.3643\n",
      "Start Epoch: 39\n",
      "Epoch 39: Average Loss: 0.3577\n",
      "Start Epoch: 40\n",
      "Epoch 40: Average Loss: 0.3452\n",
      "Start Epoch: 41\n",
      "Epoch 41: Average Loss: 0.3413\n",
      "Start Epoch: 42\n",
      "Epoch 42: Average Loss: 0.3358\n",
      "Start Epoch: 43\n",
      "Epoch 43: Average Loss: 0.3303\n",
      "Start Epoch: 44\n",
      "Epoch 44: Average Loss: 0.3250\n",
      "Start Epoch: 45\n",
      "Epoch 45: Average Loss: 0.3181\n",
      "Start Epoch: 46\n",
      "Epoch 46: Average Loss: 0.3149\n",
      "Start Epoch: 47\n",
      "Epoch 47: Average Loss: 0.3096\n",
      "Start Epoch: 48\n",
      "Epoch 48: Average Loss: 0.3024\n",
      "Start Epoch: 49\n",
      "Epoch 49: Average Loss: 0.3002\n",
      "Start Epoch: 50\n",
      "Epoch 50: Average Loss: 0.2950\n",
      "Start Epoch: 51\n",
      "Epoch 51: Average Loss: 0.2874\n",
      "Start Epoch: 52\n",
      "Epoch 52: Average Loss: 0.2848\n",
      "Start Epoch: 53\n",
      "Epoch 53: Average Loss: 0.2819\n",
      "Start Epoch: 54\n",
      "Epoch 54: Average Loss: 0.2769\n",
      "Start Epoch: 55\n",
      "Epoch 55: Average Loss: 0.2750\n",
      "Start Epoch: 56\n",
      "Epoch 56: Average Loss: 0.2702\n",
      "Start Epoch: 57\n",
      "Epoch 57: Average Loss: 0.2664\n",
      "Start Epoch: 58\n",
      "Epoch 58: Average Loss: 0.2637\n",
      "Start Epoch: 59\n",
      "Epoch 59: Average Loss: 0.2597\n",
      "Start Epoch: 60\n",
      "Epoch 60: Average Loss: 0.2554\n",
      "Start Epoch: 61\n",
      "Epoch 61: Average Loss: 0.2505\n",
      "Start Epoch: 62\n",
      "Epoch 62: Average Loss: 0.2483\n",
      "Start Epoch: 63\n",
      "Epoch 63: Average Loss: 0.2447\n",
      "Start Epoch: 64\n",
      "Epoch 64: Average Loss: 0.2437\n",
      "Start Epoch: 65\n",
      "Epoch 65: Average Loss: 0.2406\n",
      "Start Epoch: 66\n",
      "Epoch 66: Average Loss: 0.2396\n",
      "Start Epoch: 67\n",
      "Epoch 67: Average Loss: 0.2385\n",
      "Start Epoch: 68\n",
      "Epoch 68: Average Loss: 0.2364\n",
      "Start Epoch: 69\n",
      "Epoch 69: Average Loss: 0.2335\n",
      "Start Epoch: 70\n",
      "Epoch 70: Average Loss: 0.2314\n",
      "Start Epoch: 71\n",
      "Epoch 71: Average Loss: 0.2270\n",
      "Start Epoch: 72\n",
      "Epoch 72: Average Loss: 0.2251\n",
      "Start Epoch: 73\n",
      "Epoch 73: Average Loss: 0.2240\n",
      "Start Epoch: 74\n",
      "Epoch 74: Average Loss: 0.2229\n",
      "Start Epoch: 75\n",
      "Epoch 75: Average Loss: 0.2200\n",
      "Start Epoch: 76\n",
      "Epoch 76: Average Loss: 0.2172\n",
      "Start Epoch: 77\n",
      "Epoch 77: Average Loss: 0.2157\n",
      "Start Epoch: 78\n",
      "Epoch 78: Average Loss: 0.2114\n",
      "Start Epoch: 79\n",
      "Epoch 79: Average Loss: 0.2107\n",
      "Start Epoch: 80\n",
      "Epoch 80: Average Loss: 0.2082\n",
      "Start Epoch: 81\n",
      "Epoch 81: Average Loss: 0.2065\n",
      "Start Epoch: 82\n",
      "Epoch 82: Average Loss: 0.2062\n",
      "Start Epoch: 83\n",
      "Epoch 83: Average Loss: 0.2067\n",
      "Start Epoch: 84\n",
      "Epoch 84: Average Loss: 0.2058\n",
      "Start Epoch: 85\n",
      "Epoch 85: Average Loss: 0.2037\n",
      "Start Epoch: 86\n",
      "Epoch 86: Average Loss: 0.1995\n",
      "Start Epoch: 87\n",
      "Epoch 87: Average Loss: 0.1977\n",
      "Start Epoch: 88\n",
      "Epoch 88: Average Loss: 0.1963\n",
      "Start Epoch: 89\n",
      "Epoch 89: Average Loss: 0.1958\n",
      "Start Epoch: 90\n",
      "Epoch 90: Average Loss: 0.1931\n",
      "Start Epoch: 91\n",
      "Epoch 91: Average Loss: 0.1927\n",
      "Start Epoch: 92\n",
      "Epoch 92: Average Loss: 0.1899\n",
      "Start Epoch: 93\n",
      "Epoch 93: Average Loss: 0.1894\n",
      "Start Epoch: 94\n",
      "Epoch 94: Average Loss: 0.1884\n",
      "Start Epoch: 95\n",
      "Epoch 95: Average Loss: 0.1865\n",
      "Start Epoch: 96\n",
      "Epoch 96: Average Loss: 0.1856\n",
      "Start Epoch: 97\n",
      "Epoch 97: Average Loss: 0.1836\n",
      "Start Epoch: 98\n",
      "Epoch 98: Average Loss: 0.1829\n",
      "Start Epoch: 99\n",
      "Epoch 99: Average Loss: 0.1822\n",
      "Start Epoch: 100\n",
      "Epoch 100: Average Loss: 0.1810\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "EPOCHS = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print('Start Epoch:', e+1)\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_hat = model(x)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / total_batches\n",
    "    print(\"Epoch {}: Average Loss: {:.4f}\".format(e+1, avg_loss))\n",
    "\n",
    "print('Training Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "ratings['le_UserID'] = le1.transform(ratings.UserID)\n",
    "ratings['le_MovieID'] = le1.transform(ratings.MovieID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopNCF:\n",
    "    def __init__(self, model, user_encoder, movie_encoder, movie_dataset):\n",
    "        self.model = model\n",
    "        self.user_encoder = user_encoder\n",
    "        self.movie_encoder = movie_encoder\n",
    "        self.movie_dataset = movie_dataset\n",
    "\n",
    "    def recommend_movies(self, user_id, top_k=10):\n",
    "        user_tensor = torch.LongTensor([self.user_encoder.transform([user_id])[0]]).to(device)\n",
    "        movie_tensor = torch.LongTensor(range(self.movie_encoder.classes_.shape[0])).to(device)\n",
    "\n",
    "        user_tensor = user_tensor.repeat(len(movie_tensor), 1)\n",
    "\n",
    "        data = torch.cat((user_tensor, movie_tensor.unsqueeze(1)), dim=1)\n",
    "        predictions = self.model(data).detach().cpu().numpy()\n",
    "\n",
    "        top_indices = predictions.argsort(axis=0)[-top_k:][::-1]\n",
    "        top_movie_ids = self.movie_encoder.classes_[top_indices]\n",
    "\n",
    "        return top_movie_ids\n",
    "\n",
    "    def print_top_movies(self, user_id, top_k=10):\n",
    "        top_movie_ids = self.recommend_movies(user_id, top_k)\n",
    "        print(\"Top\", top_k, \"movies for user\", user_id)\n",
    "        for movie_id in top_movie_ids:\n",
    "            movie_title = self.movie_dataset.loc[self.movie_dataset['MovieID'] == movie_id, 'Title'].values[0]\n",
    "            print(f\"MovieID: {movie_id}, Title: {movie_title}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 movies for user 10\n",
      "MovieID: 2503, Title: Apple, The (Sib) (1998)\n",
      "MovieID: 53, Title: Lamerica (1994)\n",
      "MovieID: 2019, Title: Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)\n",
      "MovieID: 670, Title: World of Apu, The (Apur Sansar) (1959)\n",
      "MovieID: 318, Title: Shawshank Redemption, The (1994)\n",
      "MovieID: 3245, Title: I Am Cuba (Soy Cuba/Ya Kuba) (1964)\n",
      "MovieID: 3232, Title: Seven Chances (1925)\n",
      "MovieID: 527, Title: Schindler's List (1993)\n",
      "MovieID: 2905, Title: Sanjuro (1962)\n",
      "MovieID: 2197, Title: Firelight (1997)\n"
     ]
    }
   ],
   "source": [
    "# Load the movies dataset from a .dat file into a pandas DataFrame\n",
    "movies_cols = ['MovieID', 'Title', 'Genres']\n",
    "movies_df = pd.read_csv('./ml-1m/ml-1m/movies.dat', sep='::', engine='python', names=movies_cols, encoding='latin-1')\n",
    "\n",
    "# Assuming the movies dataset has columns ['MovieID', 'Title']\n",
    "movie_dataset = movies_df[['MovieID', 'Title']]\n",
    "\n",
    "# Usage example\n",
    "top_ncf = TopNCF(model, le1, le2, movie_dataset)\n",
    "user_id = 10\n",
    "top_ncf.print_top_movies(user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 movies for user 22\n",
      "MovieID: 501, Title: Naked (1993)\n",
      "MovieID: 2503, Title: Apple, The (Sib) (1998)\n",
      "MovieID: 2019, Title: Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)\n",
      "MovieID: 670, Title: World of Apu, The (Apur Sansar) (1959)\n",
      "MovieID: 318, Title: Shawshank Redemption, The (1994)\n",
      "MovieID: 527, Title: Schindler's List (1993)\n",
      "MovieID: 2905, Title: Sanjuro (1962)\n",
      "MovieID: 2197, Title: Firelight (1997)\n",
      "MovieID: 53, Title: Lamerica (1994)\n",
      "MovieID: 1223, Title: Grand Day Out, A (1992)\n"
     ]
    }
   ],
   "source": [
    "# Load the movies dataset from a .dat file into a pandas DataFrame\n",
    "movies_cols = ['MovieID', 'Title', 'Genres']\n",
    "movies_df = pd.read_csv('./ml-1m/ml-1m/movies.dat', sep='::', engine='python', names=movies_cols, encoding='latin-1')\n",
    "\n",
    "# Assuming the movies dataset has columns ['MovieID', 'Title']\n",
    "movie_dataset = movies_df[['MovieID', 'Title']]\n",
    "\n",
    "# Usage example\n",
    "top_ncf = TopNCF(model, le1, le2, movie_dataset)\n",
    "user_id = 22\n",
    "top_ncf.print_top_movies(user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
